import os
import pytest
import re
from collections import namedtuple
from utils_pytest import *
import warnings

from test_writable_iceberg_common import *


def test_basics(s3, pg_conn, extension):
    for table_type in ["pg_lake", "pg_lake_iceberg"]:
        internal_test_basics(s3, pg_conn, extension, table_type)


def internal_test_basics(s3, pg_conn, extension, table_type):
    location = f"s3://{TEST_BUCKET}/internal_test_basics_{table_type}/"

    pg_conn.autocommit = True

    run_command(
        f"""
        CREATE FOREIGN TABLE test_insert (
            id int not null,
            value text default 'D' collate "C",
            ser bigserial,
            gida bigint generated always as identity,
            gidd bigint generated by default as identity,
            id2 int generated always as (id * 2) stored,
            bytes bytea default '\\xdeadbeef'
        )
        SERVER {table_type}
        OPTIONS ({get_table_options(table_type)} location '{location}');

        INSERT INTO test_insert VALUES (1, 'hello');
        INSERT INTO test_insert (gidd, id) VALUES (0, 2);
        INSERT INTO test_insert (value, id) VALUES ('world', 3), ('globe', 4);
        INSERT INTO test_insert SELECT s, 'gen-'||s FROM generate_series(1,100) s;
    """,
        pg_conn,
    )

    # 4 inserts generated 4 files
    result = run_query(
        """
        SELECT count(*), sum(row_count) as row_count
        FROM lake_table.files
        WHERE table_name = 'test_insert'::regclass
    """,
        pg_conn,
    )
    assert result[0]["count"] == 4
    assert result[0]["row_count"] == 104

    # Number of rows is 104 (1+1+2+100)
    check_table_size(pg_conn, "test_insert", 104)

    # Run vacuum
    run_command("SET pg_lake_table.vacuum_compact_min_input_files TO 2", pg_conn)
    run_command("VACUUM test_insert, pg_class, test_insert", pg_conn)

    # Compacted down to 1 file
    result = run_query(
        """
        SELECT count(*), sum(row_count) as row_count
        FROM lake_table.files
        WHERE table_name = 'test_insert'::regclass AND content = 0
    """,
        pg_conn,
    )
    assert result[0]["count"] == 1
    assert result[0]["row_count"] == 104

    # Number of rows remains the same
    check_table_size(pg_conn, "test_insert", 104)

    # Do a merge-on-read deletion of 2 rows
    run_command("DELETE FROM test_insert WHERE id = 1", pg_conn)

    # Now we have 3 files total (original, deletion)
    result = run_query(
        """
        SELECT count(*)
        FROM lake_table.files
        WHERE table_name = 'test_insert'::regclass
    """,
        pg_conn,
    )
    assert result[0]["count"] == 2

    # Run vacuum (noop, because we're skipping merge-on-read)
    run_command("VACUUM test_insert", pg_conn)

    # Still have 3 files total
    result = run_query(
        """
        SELECT count(*)
        FROM lake_table.files
        WHERE table_name = 'test_insert'::regclass
    """,
        pg_conn,
    )
    assert result[0]["count"] == 2

    # Run vacuum full
    run_command("VACUUM FULL pg_class, test_insert", pg_conn)

    # Back down to 1 file
    result = run_query(
        """
        SELECT count(*), sum(row_count) as row_count
        FROM lake_table.files
        WHERE table_name = 'test_insert'::regclass
    """,
        pg_conn,
    )
    assert result[0]["count"] == 1
    assert result[0]["row_count"] == 102

    # Number of rows remains the same
    check_table_size(pg_conn, "test_insert", 102)

    run_command("DROP FOREIGN TABLE test_insert", pg_conn)
    pg_conn.autocommit = False


def test_vacuum_cancel(s3, pg_conn, extension):
    for table_type in ["pg_lake_iceberg"]:
        internal_test_vacuum_cancel(s3, pg_conn, extension, table_type)


def internal_test_vacuum_cancel(s3, pg_conn, extension, table_type):
    location = f"s3://{TEST_BUCKET}/test_vacuum_cancel/"

    run_command(
        f"""
        CREATE FOREIGN TABLE internal_test_vacuum_cancel (
            id int not null,
            value text default 'D' collate "C"
        )
        SERVER {table_type}
        OPTIONS ({get_table_options(table_type)} location '{location}');

    """,
        pg_conn,
    )

    # load some data such that VACUUM does some real job
    for i in range(0, 10):
        run_command(
            f"INSERT INTO internal_test_vacuum_cancel VALUES (1, 'hello');", pg_conn
        )

    run_command(f"SET statement_timeout TO '0.01s'", pg_conn)
    pg_conn.commit()

    pg_conn.autocommit = True

    # Clear any existing notices
    pg_conn.notices.clear()

    # cancellations are treated as WARNINGs
    error = run_command(
        f"VACUUM FULL internal_test_vacuum_cancel", pg_conn, raise_error=False
    )
    assert "canceling statement due to statement timeout" in error

    pg_conn.autocommit = False
    run_command(f"RESET statement_timeout", pg_conn)
    run_command("DROP FOREIGN TABLE internal_test_vacuum_cancel", pg_conn)
    pg_conn.commit()


def test_partitioning(s3, pg_conn, extension):
    for table_type in ["pg_lake", "pg_lake_iceberg"]:
        internal_test_partitioning(pg_conn, s3, extension, table_type)


def internal_test_partitioning(pg_conn, s3, extension, table_type):
    location = f"s3://{TEST_BUCKET}/internal_test_partitioning_{table_type}"

    pg_conn.autocommit = True

    # Create a partitioned table with 2 partitions (0-999, 1000-1999)
    run_command(
        f"""
        CREATE TABLE test_partitioned (
            id int not null check (id > 0),
            value text default 'D' not null,
            ser bigserial,
            gida bigint generated always as identity,
            gidd bigint generated by default as identity,
            id2 int generated always as (id * 2) stored
        )
        PARTITION BY RANGE (id);

		CREATE FOREIGN TABLE test_partitioned_1
        PARTITION OF test_partitioned
        FOR VALUES FROM (0) TO (1001)
        SERVER {table_type}
        OPTIONS ({get_table_options(table_type)} location '{location}_1');

		CREATE FOREIGN TABLE test_partitioned_2
        PARTITION OF test_partitioned
        FOR VALUES FROM (1001) TO (3000)
        SERVER {table_type}
        OPTIONS ({get_table_options(table_type)} location '{location}_2');
    """,
        pg_conn,
    )

    run_command(
        f"""INSERT INTO test_partitioned VALUES (1, 'hello');
        INSERT INTO test_partitioned (gidd, id) VALUES (0, 2);
        INSERT INTO test_partitioned (value, id) VALUES ('world', 3), ('globe', 4);
        INSERT INTO test_partitioned (value, id)
        SELECT 'hello-'||s, s FROM generate_series(1,2000) s;
    """,
        pg_conn,
    )

    # 4 inserts generated 4 files
    result = run_query(
        """
        SELECT count(*), sum(row_count) as row_count
        FROM lake_table.files
        WHERE table_name = 'test_partitioned_1'::regclass
    """,
        pg_conn,
    )
    assert result[0]["count"] == 4
    assert result[0]["row_count"] == 1004

    # Run vacuum on parent
    run_command("VACUUM test_partitioned", pg_conn)

    # files are compacted together
    result = run_query(
        """
        SELECT count(*), sum(row_count) as row_count
        FROM lake_table.files
        WHERE table_name = 'test_partitioned_1'::regclass
    """,
        pg_conn,
    )
    assert result[0]["count"] == 1
    assert result[0]["row_count"] == 1004

    check_table_size(pg_conn, "test_partitioned", 2004)

    run_command("DROP TABLE test_partitioned", pg_conn)
    pg_conn.autocommit = False


# We use superuser to bypass target_file_size_mb lower bound
def test_splitting(superuser_conn, s3, extension):
    location = f"s3://{TEST_BUCKET}/test_vacuum_splitting/"

    superuser_conn.autocommit = True

    run_command(
        f"""
        CREATE FOREIGN TABLE test_splitting (
            id bigserial,
            value text
        )
        SERVER pg_lake
        OPTIONS (writable 'true', format 'parquet', location '{location}');

        -- Generate 2 * ~18MB files
        INSERT INTO test_splitting (value)
        SELECT md5(s::text) FROM generate_series(1,550000) s;
        INSERT INTO test_splitting (value)
        SELECT md5(s::text) FROM generate_series(1,550000) s;
    """,
        superuser_conn,
    )

    # Got a single file
    result = run_query(
        """
        SELECT count(*)
        FROM lake_table.files
        WHERE table_name = 'test_splitting'::regclass
    """,
        superuser_conn,
    )
    assert result[0]["count"] == 2

    # We would like smaller files
    run_command(
        """
        SET pg_lake_table.target_file_size_mb TO '3MB';
    """,
        superuser_conn,
    )

    # We will do several compaction iterations, since files exceed 4 * target size
    run_command(
        """
        VACUUM test_splitting;
    """,
        superuser_conn,
    )

    # Got many files
    result = run_query(
        """
        SELECT count(*)
        FROM lake_table.files
        WHERE table_name = 'test_splitting'::regclass
    """,
        superuser_conn,
    )
    assert result[0]["count"] >= 4

    # Cleanup
    run_command(
        """
         RESET pg_lake_table.target_file_size_mb;
         DROP FOREIGN TABLE test_splitting;
    """,
        superuser_conn,
    )
    superuser_conn.autocommit = False


def test_vacuum_all_iceberg_tables(pg_conn, s3, extension, with_default_location):
    pg_conn.autocommit = True
    run_command(f"""CREATE SCHEMA "sc 1";""", pg_conn)

    for i in range(0, 2):
        run_command(
            f"""
            CREATE TABLE "sc 1"."test insert {i}" (
                id int not null,
                value text default 'D' collate "C"
            )
            USING pg_lake_iceberg;
            ALTER FOREIGN TABLE "sc 1"."test insert {i}" OPTIONS (ADD autovacuum_enabled 'false');

            INSERT INTO "sc 1"."test insert {i}" VALUES (1, 'hello');
            INSERT INTO "sc 1"."test insert {i}" VALUES (2, 'bye');
            INSERT INTO "sc 1"."test insert {i}" (value, id) VALUES ('world', 3), ('globe', 4);
            INSERT INTO "sc 1"."test insert {i}" SELECT s, 'gen-'||s FROM generate_series(1,100) s;
        """,
            pg_conn,
        )
    pg_conn.commit()

    # 4 inserts generated 4 files
    for i in range(0, 2):
        result = run_query(
            f"""
            SELECT count(*) as cnt
            FROM lake_table.files
            WHERE table_name = '"sc 1"."test insert {i}"'::regclass
        """,
            pg_conn,
        )
        assert result[0]["cnt"] == 4

    # Run vacuum
    run_command_outside_tx(
        [
            "SET pg_lake_table.vacuum_compact_min_input_files TO 1",
            "VACUUM (IceberG)",
        ],
        pg_conn,
    )

    # Compacted down to 1 file
    for i in range(0, 2):
        result = run_query(
            f"""
            SELECT count(*) as cnt
            FROM lake_table.files
            WHERE table_name = '"sc 1"."test insert {i}"'::regclass AND content = 0
        """,
            pg_conn,
        )
        assert result[0]["cnt"] == 1

    # pg_lake_iceberg with a table name is not supported
    try:
        run_command_outside_tx(
            [f'''VACUUM (Iceberg) "sc 1"."test insert 0"'''], pg_conn
        )

        # never expect to come here
        assert False
    except psycopg2.DatabaseError as error:
        assert "does not accept a list of relations" in str(error)


def test_vacuum_columns(s3, pg_conn, extension):
    location = f"s3://{TEST_BUCKET}/test_vacuum_columns/"

    run_command(
        f"""
        CREATE FOREIGN TABLE test_vacuum_columns (
            id int,
            value text
        )
        SERVER pg_lake_iceberg
        OPTIONS (location '{location}');
    """,
        pg_conn,
    )

    pg_conn.autocommit = True

    # VACUUM ANALYZE with column name is ok (but has no special effect)
    run_command(
        f"""
        VACUUM ANALYZE test_vacuum_columns (id);
    """,
        pg_conn,
    )

    # VACUUM with column names throws a hard error in postgres
    error = run_command(
        f"""
        VACUUM test_vacuum_columns (id);
    """,
        pg_conn,
        raise_error=False,
    )
    assert "ANALYZE option must be specified" in error

    pg_conn.rollback()
    pg_conn.autocommit = False


def test_vacuum_verbose(s3, pg_conn, extension):
    location = f"s3://{TEST_BUCKET}/test_vacuum_verbose/"

    pg_conn.autocommit = True

    run_command(
        f"""
        CREATE TABLE test_vacuum_verbose (
            id int,
            value text
        )
        USING pg_lake_iceberg
        WITH (location = '{location}');
        INSERT INTO test_vacuum_verbose VALUES (1, 'hello');
        INSERT INTO test_vacuum_verbose VALUES (2, 'world');
        SET pg_lake_iceberg.max_snapshot_age TO 0;
        SET pg_lake_table.vacuum_compact_min_input_files TO 1;
    """,
        pg_conn,
    )

    # Clear any existing notices
    pg_conn.notices.clear()

    run_command(
        f"""
        VACUUM VERBOSE test_vacuum_verbose;
    """,
        pg_conn,
    )

    assert any("adding" in line for line in pg_conn.notices)
    assert any("removing" in line for line in pg_conn.notices)
    assert any("expiring" in line for line in pg_conn.notices)

    run_command(
        f"""
        RESET pg_lake_iceberg.max_snapshot_age;
        RESET pg_lake_table.vacuum_compact_min_input_files;
        DROP TABLE test_vacuum_verbose;
    """,
        pg_conn,
    )

    pg_conn.autocommit = False


def test_vacuum_multiple_metadata_ops(s3, pg_conn, extension, with_default_location):
    pg_conn.autocommit = True

    run_command(
        f"""
            SET pg_lake_iceberg.enable_manifest_merge_on_write TO off;
            SET pg_lake_iceberg.manifest_min_count_to_merge TO 2;
            SET pg_lake_iceberg.max_snapshot_age TO 0;

            CREATE TABLE test_vacuum_multiple_metadata_ops(key int) USING iceberg;

            INSERT INTO test_vacuum_multiple_metadata_ops VALUES (1);
    """,
        pg_conn,
    )

    run_command("INSERT INTO test_vacuum_multiple_metadata_ops VALUES (2);", pg_conn)

    # Clear any existing notices
    pg_conn.notices.clear()

    run_command(
        f"""
        VACUUM (verbose) test_vacuum_multiple_metadata_ops;
    """,
        pg_conn,
    )

    assert any("merging 2 manifests into" in line for line in pg_conn.notices)

    # triggers a single snapshot, we only have 2 snapshot expiring
    # for INSERTs
    assert sum("expiring" in line for line in pg_conn.notices) == 2

    run_command(
        f"""
        RESET pg_lake_iceberg.enable_manifest_merge_on_write;
        RESET pg_lake_iceberg.manifest_min_count_to_merge;
        RESET pg_lake_iceberg.max_snapshot_age;
        DROP TABLE test_vacuum_multiple_metadata_ops;
    """,
        pg_conn,
    )

    pg_conn.autocommit = False


def test_vacuum_iceberg_with_dropped_table(s3, superuser_conn, extension):
    location = f"s3://{TEST_BUCKET}/test_vacuum_iceberg_with_dropped_table/"

    superuser_conn.autocommit = True

    run_command(
        f"""
        CREATE TABLE test_vacuum_iceberg_with_dropped_table (
            id int,
            value text
        )
        USING pg_lake_iceberg
        WITH (location = '{location}');
        INSERT INTO test_vacuum_iceberg_with_dropped_table VALUES (1, 'hello');
        INSERT INTO test_vacuum_iceberg_with_dropped_table VALUES (2, 'world');
    """,
        superuser_conn,
    )
    superuser_conn.commit()

    # create in progress and deletion queue rows
    run_command("TRUNCATE test_vacuum_iceberg_with_dropped_table", superuser_conn)
    superuser_conn.commit()

    run_command(
        "BEGIN; INSERT INTO test_vacuum_iceberg_with_dropped_table VALUES (1, 'test'); ROLLBACK;",
        superuser_conn,
    )

    run_command("set pg_lake_table.skip_drop_access_hook to on;", superuser_conn)
    run_command("DROP TABLE test_vacuum_iceberg_with_dropped_table", superuser_conn)
    superuser_conn.commit()

    # should not crash
    run_command(
        f"""
        VACUUM (iceberg);
    """,
        superuser_conn,
    )

    superuser_conn.autocommit = False
    run_command("reset pg_lake_table.skip_drop_access_hook;", superuser_conn)


def test_vacuum_tx_block(s3, pg_conn, extension):
    for table_type in ["pg_lake", "pg_lake_iceberg"]:
        internal_test_vacuum_tx_block(s3, pg_conn, extension, table_type)


def internal_test_vacuum_tx_block(s3, pg_conn, extension, table_type):
    location = f"s3://{TEST_BUCKET}/internal_test_vacuum_tx_block{table_type}/"

    run_command(
        f"""
        CREATE SCHEMA internal_test_vacuum_tx_block;
        CREATE FOREIGN TABLE internal_test_vacuum_tx_block.test_insert (
            id int not null
        )
        SERVER {table_type}
        OPTIONS ({get_table_options(table_type)} location '{location}');

        INSERT INTO internal_test_vacuum_tx_block.test_insert VALUES (1);

            CREATE OR REPLACE FUNCTION internal_test_vacuum_tx_block.check_vacuum()
            RETURNS void
            LANGUAGE plpgsql
            AS $$
            BEGIN
                
                  VACUUM test_insert;

            END;
        $$;
    """,
        pg_conn,
    )
    pg_conn.commit()

    error = run_command(
        "VACUUM internal_test_vacuum_tx_block.test_insert", pg_conn, raise_error=False
    )
    assert "VACUUM cannot run inside a transaction block" in error
    pg_conn.rollback()

    error = run_command(
        "SELECT internal_test_vacuum_tx_block.check_vacuum()",
        pg_conn,
        raise_error=False,
    )
    assert "VACUUM cannot run inside a transaction block" in error
    pg_conn.rollback()

    # no tx block, we can still detect
    pg_conn.autocommit = True
    error = run_command(
        "SELECT internal_test_vacuum_tx_block.check_vacuum()",
        pg_conn,
        raise_error=False,
    )
    assert "VACUUM cannot be executed from a function" in error
    pg_conn.rollback()
    pg_conn.autocommit = False

    run_command("DROP SCHEMA internal_test_vacuum_tx_block CASCADE", pg_conn)
    pg_conn.commit()


def test_vacuum_delete_threshold(s3, pg_conn, extension, with_default_location):
    location = f"s3://{TEST_BUCKET}/test_vacuum_delete_threshold/"

    run_command(
        f"""
        CREATE TABLE test_vacuum_delete_threshold (
            id int,
            value text
        )
        USING pg_lake_iceberg;
        INSERT INTO test_vacuum_delete_threshold SELECT s, 'hello' FROM generate_series(1,10) s;

        SET pg_lake_table.vacuum_compact_min_input_files TO 1;

        -- Do a large deletion, but use position delete
        SET pg_lake_table.copy_on_write_threshold TO 80;
        DELETE FROM test_vacuum_delete_threshold WHERE id > 5;
        RESET pg_lake_table.copy_on_write_threshold;
    """,
        pg_conn,
    )

    # should have a position delete file
    result = run_query(
        """
        SELECT count(*)
        FROM lake_table.files
        WHERE table_name = 'test_vacuum_delete_threshold'::regclass
        AND content = 1
    """,
        pg_conn,
    )
    assert result[0]["count"] == 1

    # commit to be able to vacuum
    pg_conn.commit()

    pg_conn.autocommit = True

    # should trigger compaction based on pg_lake_table.copy_on_write_threshold
    run_command(
        f"""
        VACUUM test_vacuum_delete_threshold;
    """,
        pg_conn,
    )

    pg_conn.autocommit = False

    # should have merged the deletions
    result = run_query(
        """
        SELECT count(*)
        FROM lake_table.files
        WHERE table_name = 'test_vacuum_delete_threshold'::regclass
        AND content = 1
    """,
        pg_conn,
    )
    assert result[0]["count"] == 0

    run_command("DROP TABLE test_vacuum_delete_threshold CASCADE", pg_conn)
    pg_conn.commit()


def test_vacuum_iceberg_identity_partitioned_table(
    s3, pg_conn, extension, grant_access_to_data_file_partition, with_default_location
):
    table_name = "test_vacuum_iceberg_partitioned_table"

    # create an identity partitioned table and insert some data
    run_command(
        f"""
        CREATE TABLE {table_name} (
            id int,
            value text
        )
        USING pg_lake_iceberg WITH (partition_by = 'id');
        INSERT INTO {table_name} SELECT s, 'hello' FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT s, 'hello' FROM generate_series(1,10) s;

        SET pg_lake_table.vacuum_compact_min_input_files TO 1;
    """,
        pg_conn,
    )

    # all unique ids are in the different partitions (duplicates are in the same partition)
    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=10,
        expected_data_files_cnt=10,
    )

    # drop partition_by and insert with empty spec (files with empty spec are compacted)
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (DROP partition_by);
        INSERT INTO {table_name} VALUES (1, 'hello');
        INSERT INTO {table_name} VALUES (2, 'world');
        """,
        pg_conn,
    )

    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=10,
        expected_data_files_cnt=11,
    )

    # add same partition_by and insert with the existing spec
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (ADD partition_by 'id');
        INSERT INTO {table_name} SELECT s, 'hello' FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT s, 'hello' FROM generate_series(1,10) s;
        """,
        pg_conn,
    )

    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=10,
        expected_data_files_cnt=11,
    )

    # set new partition_by and insert with a new spec
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (SET partition_by 'value');
        INSERT INTO {table_name} SELECT s, 'hello' || s FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT s, 'hello' || s FROM generate_series(1,10) s;
        """,
        pg_conn,
    )

    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=20,
        expected_data_files_cnt=21,
    )

    # sanity check on table
    cnt = run_query(f"select count(*) from {table_name}", pg_conn)[0][0]
    assert cnt == 62

    run_command(f"DROP TABLE {table_name} CASCADE", pg_conn)
    pg_conn.commit()


def test_vacuum_iceberg_truncate_partitioned_table(
    s3, pg_conn, extension, grant_access_to_data_file_partition, with_default_location
):
    table_name = "test_vacuum_iceberg_truncate_partitioned_table"

    # create a truncate partitioned table and insert some data
    run_command(
        f"""
        CREATE TABLE {table_name} (
            id text,
            value text
        )
        USING pg_lake_iceberg WITH (partition_by = 'truncate(1,id)');
        INSERT INTO {table_name} SELECT s, 'hello' FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT s, 'hello' FROM generate_series(1,10) s;

        SET pg_lake_table.vacuum_compact_min_input_files TO 1;
    """,
        pg_conn,
    )

    # only '1' and '10' are in the same partition ('10' is truncated to '1')
    vacuum_table(pg_conn, table_name, is_full=True)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=9,
        expected_data_files_cnt=9,
    )

    # drop partition_by and insert with empty spec (files with empty spec are compacted)
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (DROP partition_by);
        INSERT INTO {table_name} VALUES (1, 'hello');
        INSERT INTO {table_name} VALUES (2, 'world');
        """,
        pg_conn,
    )

    vacuum_table(pg_conn, table_name, is_full=True)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=9,
        expected_data_files_cnt=10,
    )

    # add same partition_by and insert with a the old spec
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (ADD partition_by 'truncate(1,id)');
        INSERT INTO {table_name} SELECT s, 'hello' FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT s, 'hello' FROM generate_series(1,10) s;
        """,
        pg_conn,
    )

    # all new files are in the different partitions except '1' and '10'
    vacuum_table(pg_conn, table_name, is_full=True)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=9,
        expected_data_files_cnt=10,
    )

    # set new partition_by and insert with a new spec
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (SET partition_by 'truncate(6,value)');
        INSERT INTO {table_name} SELECT s, 'hello' || s FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT s, 'hello' || s FROM generate_series(1,10) s;
        """,
        pg_conn,
    )

    # all new files are in the different partitions except 'hello1' and 'hello10'
    vacuum_table(pg_conn, table_name, is_full=True)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=18,
        expected_data_files_cnt=19,
    )

    # sanity check on table
    cnt = run_query(f"select count(*) from {table_name}", pg_conn)[0][0]
    assert cnt == 62

    run_command(f"DROP TABLE {table_name} CASCADE", pg_conn)
    pg_conn.commit()


def test_vacuum_iceberg_bucket_partitioned_table(
    s3, pg_conn, extension, grant_access_to_data_file_partition, with_default_location
):
    table_name = "test_vacuum_iceberg_bucket_partitioned_table"

    # create a bucket partitioned table and insert some data
    run_command(
        f"""
        CREATE TABLE {table_name} (
            id text,
            value text
        )
        USING pg_lake_iceberg WITH (partition_by = 'bucket(100,id)');
        INSERT INTO {table_name} SELECT s, 'hello' FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT s, 'hello' FROM generate_series(1,10) s;

        SET pg_lake_table.vacuum_compact_min_input_files TO 1;
    """,
        pg_conn,
    )

    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=8,
        expected_data_files_cnt=8,
    )

    # drop partition_by and insert with empty spec
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (DROP partition_by);
        INSERT INTO {table_name} VALUES (1, 'hello');
        INSERT INTO {table_name} VALUES (2, 'world');
        """,
        pg_conn,
    )

    # files with empty spec are compacted
    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=8,
        expected_data_files_cnt=9,
    )

    # add same partition_by and insert with a the existing spec
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (ADD partition_by 'bucket(100,id)');
        INSERT INTO {table_name} SELECT s, 'hello' FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT s, 'hello' FROM generate_series(1,10) s;
        """,
        pg_conn,
    )

    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=8,
        expected_data_files_cnt=9,
    )

    # set new partition_by and insert with a new spec
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (SET partition_by 'bucket(100,value)');
        INSERT INTO {table_name} SELECT s, 'hello' || s FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT s, 'hello' || s FROM generate_series(1,10) s;
        """,
        pg_conn,
    )

    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=16,
        expected_data_files_cnt=17,
    )

    # sanity check on table
    cnt = run_query(f"select count(*) from {table_name}", pg_conn)[0][0]
    assert cnt == 62

    run_command(f"DROP TABLE {table_name} CASCADE", pg_conn)
    pg_conn.commit()


def test_vacuum_iceberg_year_partitioned_table(
    s3, pg_conn, extension, grant_access_to_data_file_partition, with_default_location
):
    table_name = "test_vacuum_iceberg_year_partitioned_table"

    # create a year partitioned table and insert some data
    run_command(
        f"""
        CREATE TABLE {table_name} (
            a date,
            b timestamp
        )
        USING pg_lake_iceberg WITH (partition_by = 'year(a)');
        INSERT INTO {table_name} SELECT ('20' || s || '-' || s || '-' || s)::date,
                                        ('20' || s || '-' || s || '-' || s || ' ' || s || ':' || s || ':' || s)::timestamp
                                 FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT ('20' || s || '-' || s || '-' || s)::date,
                                        ('20' || s || '-' || s || '-' || s || ' ' || s || ':' || s || ':' || s)::timestamp
                                 FROM generate_series(1,10) s;

        SET pg_lake_table.vacuum_compact_min_input_files TO 1;
    """,
        pg_conn,
    )

    # all files are in the different partitions
    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=10,
        expected_data_files_cnt=10,
    )

    # drop partition_by and insert with empty spec
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (DROP partition_by);
        INSERT INTO {table_name} SELECT ('20' || s || '-' || s || '-' || s)::date,
                                        ('20' || s || '-' || s || '-' || s || ' ' || s || ':' || s || ':' || s)::timestamp
                                 FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT ('20' || s || '-' || s || '-' || s)::date,
                                        ('20' || s || '-' || s || '-' || s || ' ' || s || ':' || s || ':' || s)::timestamp
                                 FROM generate_series(1,10) s;
        """,
        pg_conn,
    )

    # files with empty spec are compacted
    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=10,
        expected_data_files_cnt=11,
    )

    # add same partition_by and insert with the old spec
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (ADD partition_by 'year(a)');
        INSERT INTO {table_name} SELECT ('20' || s || '-' || s || '-' || s)::date,
                                        ('20' || s || '-' || s || '-' || s || ' ' || s || ':' || s || ':' || s)::timestamp
                                 FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT ('20' || s || '-' || s || '-' || s)::date,
                                        ('20' || s || '-' || s || '-' || s || ' ' || s || ':' || s || ':' || s)::timestamp
                                 FROM generate_series(1,10) s;
        """,
        pg_conn,
    )

    # all new files are in the different partitions
    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=10,
        expected_data_files_cnt=11,
    )

    # set new partition_by and insert with a new spec
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (SET partition_by 'hour(b)');
        INSERT INTO {table_name} SELECT ('20' || s || '-' || s || '-' || s)::date,
                                        ('20' || s || '-' || s || '-' || s || ' ' || s || ':' || s || ':' || s)::timestamp
                                 FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT ('20' || s || '-' || s || '-' || s)::date,
                                        ('20' || s || '-' || s || '-' || s || ' ' || s || ':' || s || ':' || s)::timestamp
                                 FROM generate_series(1,10) s;
        """,
        pg_conn,
    )

    # all new files are in the different partitions
    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=20,
        expected_data_files_cnt=21,
    )

    # sanity check on table
    cnt = run_query(f"select count(*) from {table_name}", pg_conn)[0][0]
    assert cnt == 80

    run_command(f"DROP TABLE {table_name} CASCADE", pg_conn)
    pg_conn.commit()


def test_vacuum_iceberg_multi_partitioned_table(
    s3, pg_conn, extension, grant_access_to_data_file_partition, with_default_location
):
    table_name = "test_vacuum_iceberg_multi_partitioned_table"

    # create a multi partitioned table and insert some data
    run_command(
        f"""
        CREATE TABLE {table_name} (
            id text,
            value text
        )
        USING pg_lake_iceberg WITH (partition_by = 'truncate(1,id), bucket(100,value)');
        INSERT INTO {table_name} SELECT s, 'hello' || s FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT s, 'hello' || s FROM generate_series(1,10) s;

        SET pg_lake_table.vacuum_compact_min_input_files TO 1;
    """,
        pg_conn,
    )

    # all rows are in the different partitions (duplicates are in the same partition)
    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=20,
        expected_data_files_cnt=10,
    )

    # drop partition_by and insert with empty spec (files with empty spec are compacted)
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (DROP partition_by);
        INSERT INTO {table_name} VALUES (1, 'hello');
        INSERT INTO {table_name} VALUES (2, 'world');
        """,
        pg_conn,
    )

    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=20,
        expected_data_files_cnt=11,
    )

    # add same partition_by and insert with the same spec
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (ADD partition_by 'truncate(1,id), bucket(100,value)');
        INSERT INTO {table_name} SELECT s, 'hello' || s FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT s, 'hello' || s FROM generate_series(1,10) s;
        """,
        pg_conn,
    )

    # all rows are in the different partitions (duplicates are in the same partition)
    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=20,
        expected_data_files_cnt=11,
    )

    # set new partition_by and insert with a new spec
    run_command(
        f"""
        ALTER TABLE {table_name} OPTIONS (SET partition_by 'truncate(1,value), bucket(1, value)');
        INSERT INTO {table_name} SELECT s, 'hello' || s FROM generate_series(1,10) s;
        INSERT INTO {table_name} SELECT s, 'hello' || s FROM generate_series(1,10) s;
        """,
        pg_conn,
    )

    # all new files are in the different partitions except 'hello1' and 'hello10'
    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=22,
        expected_data_files_cnt=12,
    )

    # sanity check on table
    cnt = run_query(f"select count(*) from {table_name}", pg_conn)[0][0]
    assert cnt == 62

    run_command(f"DROP TABLE {table_name} CASCADE", pg_conn)
    pg_conn.commit()


def test_vacuum_copy_on_write_partitioned_table(
    s3, pg_conn, extension, grant_access_to_data_file_partition, with_default_location
):
    table_name = "test_vacuum_copy_on_write_partitioned_table"

    # create a partitioned table and insert some data
    run_command(
        f"""
        CREATE TABLE {table_name} (
            a int
        )
        USING pg_lake_iceberg WITH (partition_by = 'bucket(2,a)');
        INSERT INTO {table_name} SELECT s FROM generate_series(1,10) s;

        SET pg_lake_table.vacuum_compact_min_input_files TO 1;
        
        -- Do a large deletion, but use position delete
        SET pg_lake_table.copy_on_write_threshold TO 100;
        DELETE FROM {table_name} WHERE a > 5;
        RESET pg_lake_table.copy_on_write_threshold;
    """,
        pg_conn,
    )

    # should have position delete file per partition (total 2)
    result = run_query(
        f"""
        SELECT count(*)
        FROM lake_table.files
        WHERE table_name = '{table_name}'::regclass
        AND content = 1
        """,
        pg_conn,
    )
    assert result[0]["count"] == 2

    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=4,
        expected_data_files_cnt=4,
    )

    vacuum_table(pg_conn, table_name)

    # should have merged the deletions
    result = run_query(
        f"""
        SELECT count(*)
        FROM lake_table.files
        WHERE table_name = '{table_name}'::regclass
        AND content = 1
        """,
        pg_conn,
    )
    assert result[0]["count"] == 0

    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=2,
        expected_data_files_cnt=2,
    )

    # sanity check on table
    cnt = run_query(f"select count(*) from {table_name}", pg_conn)[0][0]
    assert cnt == 5

    run_command(f"DROP TABLE {table_name} CASCADE", pg_conn)
    pg_conn.commit()


def test_vacuum_max_compaction_count(
    s3, pg_conn, extension, grant_access_to_data_file_partition, with_default_location
):
    table_name = "test_vacuum_max_compaction_count"

    # create an identity partitioned table and insert some data
    run_command(
        f"""
        CREATE TABLE {table_name} (
            a int
        )
        USING pg_lake_iceberg WITH (partition_by = 'a');
        
        -- needs 3 compactions in total (3 separate partitions)
        INSERT INTO {table_name} VALUES (1);
        INSERT INTO {table_name} VALUES (1);
        INSERT INTO {table_name} VALUES (2);
        INSERT INTO {table_name} VALUES (2);
        INSERT INTO {table_name} VALUES (3);
        INSERT INTO {table_name} VALUES (3);

        SET pg_lake_table.vacuum_compact_min_input_files TO 1;
    """,
        pg_conn,
    )

    run_command("SET pg_lake_table.max_compactions_per_vacuum TO 1;", pg_conn)

    # initially nothing merged
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=6,
        expected_data_files_cnt=6,
    )

    # only 1 partition should be merged
    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=5,
        expected_data_files_cnt=5,
    )

    # all partitions should be merged
    run_command("RESET pg_lake_table.max_compactions_per_vacuum;", pg_conn)
    vacuum_table(pg_conn, table_name)
    assert_partition_values_and_data_files(
        pg_conn,
        table_name,
        expected_partition_values_cnt=3,
        expected_data_files_cnt=3,
    )

    # sanity check on table
    cnt = run_query(f"select count(*) from {table_name}", pg_conn)[0][0]
    assert cnt == 6

    run_command(f"DROP TABLE {table_name} CASCADE", pg_conn)
    pg_conn.commit()


def test_target_row_group_size(pg_conn, duckdb_conn, extension, tmp_path):
    parquet_path = tmp_path / "test.parquet"

    run_command("create table test_target_row_group_size(a text);", pg_conn)

    run_command(
        """
                INSERT INTO test_target_row_group_size SELECT ( 
                    WITH chars AS ( SELECT substr('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789', floor(random()*62 + 1)::int, 1) AS ch
                                     FROM generate_series(1, 20000))
                    SELECT string_agg(ch, '') AS random_text FROM chars
                ) from generate_series(1, 20000) i;                
                """,
        pg_conn,
    )

    copy_command = (
        f"COPY test_target_row_group_size TO '{parquet_path}' WITH (format 'parquet');"
    )

    # copy with row_group_size_bytes disabled
    run_command("SET pg_lake_table.target_row_group_size_mb to 0;", pg_conn)

    run_command(copy_command, pg_conn)

    duckdb_conn.execute(
        "SELECT count(*) FROM parquet_metadata($1)", [str(parquet_path)]
    )
    row_groups_default = duckdb_conn.fetchall()[0][0]

    # default row_group_size = ~122k rows so we expect 1 row group for 20k rows by default
    assert row_groups_default == 1

    # copy with row_group_size_bytes = 2MB
    run_command("SET pg_lake_table.target_row_group_size_mb to '2MB';", pg_conn)

    run_command(copy_command, pg_conn)

    duckdb_conn.execute(
        "SELECT count(*) FROM parquet_metadata($1)", [str(parquet_path)]
    )
    row_groups_2mb = duckdb_conn.fetchall()[0][0]

    # copy with row_group_size_bytes = 128MB
    run_command("SET pg_lake_table.target_row_group_size_mb to '128MB';", pg_conn)

    run_command(copy_command, pg_conn)

    duckdb_conn.execute(
        "SELECT count(*) FROM parquet_metadata($1)", [str(parquet_path)]
    )
    row_groups_128mb = duckdb_conn.fetchall()[0][0]

    assert row_groups_2mb > row_groups_128mb > row_groups_default

    pg_conn.rollback()


def get_table_options(table_type):
    if table_type == "pg_lake":
        return "writable 'true', format 'parquet',"
    elif table_type == "pg_lake_iceberg":
        return "autovacuum_enabled 'false', "


def assert_partition_values_and_data_files(
    pg_conn, table_name, expected_partition_values_cnt, expected_data_files_cnt
):
    partition_values_cnt = run_query(
        f"""
        select count(*) from lake_table.data_file_partition_values where table_name = '{table_name}'::regclass;
    """,
        pg_conn,
    )[0][0]
    assert partition_values_cnt == expected_partition_values_cnt

    data_files_cnt = run_query(
        f"""
        select count(*) from lake_table.files where table_name = '{table_name}'::regclass;
    """,
        pg_conn,
    )[0][0]
    assert data_files_cnt == expected_data_files_cnt


def vacuum_table(pg_conn, table_name, is_full=False):
    old_auto_commit = pg_conn.autocommit

    pg_conn.commit()
    pg_conn.autocommit = True

    # all new files are in the different partitions
    run_command(
        f"""
        VACUUM {'FULL' if is_full else ''} {table_name};
        """,
        pg_conn,
    )

    pg_conn.autocommit = old_auto_commit
